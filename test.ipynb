{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import Audio, load_dataset\n",
    "import pandas as pd\n",
    "import librosa"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T10:16:40.002524914Z",
     "start_time": "2023-05-05T10:16:36.877211640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T10:17:02.233594597Z",
     "start_time": "2023-05-05T10:16:40.004628325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvel/anaconda3/envs/wav2vec2/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bonsoir.']\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "path = 'audio_samples/train_0023.wav'\n",
    "input_speech, _ = librosa.load(path, sr=None)\n",
    "input_features = processor(input_speech, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T10:17:19.537828552Z",
     "start_time": "2023-05-05T10:17:09.096125790Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transcription_df = pd.DataFrame(columns=['wav', 'transcription'])\n",
    "for wav in ['audio_samples/train_0001.wav', 'audio_samples/train_0002.wav', 'audio_samples/train_0003.wav']:\n",
    "    input_speech, _ = librosa.load(wav, sr=None)\n",
    "    input_features = processor(input_speech, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    # generate token ids\n",
    "    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    dict_metadata = {\n",
    "        'wav': wav,\n",
    "        'transcription': transcription\n",
    "    }\n",
    "    transcription_df = transcription_df.append(dict_metadata, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/wav2vec2/lib/python3.9/site-packages/pandas/core/indexes/range.py:385\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_range\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[0;31mValueError\u001B[0m: 3 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [45], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m transcription_df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecipes/requests/data/transcripts/train_transcriptions.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtranscription_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtranscription\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/envs/wav2vec2/lib/python3.9/site-packages/pandas/core/series.py:958\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    955\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[0;32m--> 958\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    963\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/wav2vec2/lib/python3.9/site-packages/pandas/core/series.py:1069\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[0;34m(self, label, takeable)\u001B[0m\n\u001B[1;32m   1066\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[0;32m-> 1069\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "File \u001B[0;32m~/anaconda3/envs/wav2vec2/lib/python3.9/site-packages/pandas/core/indexes/range.py:387\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_range\u001B[38;5;241m.\u001B[39mindex(new_key)\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 387\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 3"
     ]
    }
   ],
   "source": [
    "transcription_df.to_csv('recipes/requests/data/transcripts/train_transcriptions.csv', index=False)\n",
    "print(transcription_df['transcription'][3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T09:28:11.618015076Z",
     "start_time": "2023-05-05T09:28:11.510447152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "df = pd.read_csv('recipes/requests/data/transcripts/train_transcriptions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T14:21:37.838102580Z",
     "start_time": "2023-05-04T14:21:37.777224214Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip pyctcdecode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b36aac03744d4f13ae1f2c31ee9d3649"
      },
      "application/json": {
       "n": 0,
       "total": 1009,
       "elapsed": 0.02093029022216797,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5de1fa7bcda140c08557a0506573ea3a"
      },
      "application/json": {
       "n": 0,
       "total": 3086784939,
       "elapsed": 0.024135351181030273,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading pytorch_model.bin",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "003da73ed03f4477afb1277184a99a5d"
      },
      "application/json": {
       "n": 0,
       "total": 184989,
       "elapsed": 0.017173051834106445,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)rocessor_config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/832 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbdd635c1c304fdeaeea2e8f54db9c1a"
      },
      "application/json": {
       "n": 0,
       "total": 832,
       "elapsed": 0.012210369110107422,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f657285eb6d94d7a854a706c2d0b75f3"
      },
      "application/json": {
       "n": 0,
       "total": 1036558,
       "elapsed": 0.02238917350769043,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)olve/main/vocab.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b582077b73a847ebb7d91eb3946019a6"
      },
      "application/json": {
       "n": 0,
       "total": 493864,
       "elapsed": 0.024315834045410156,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)olve/main/merges.txt",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.11k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "710e73676ef04b8cb26030ad0a5464f6"
      },
      "application/json": {
       "n": 0,
       "total": 2108,
       "elapsed": 0.020200729370117188,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)in/added_tokens.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.06k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac5d6e6c7ad245cabbea00f673fab63c"
      },
      "application/json": {
       "n": 0,
       "total": 2064,
       "elapsed": 0.02538466453552246,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur les semelles orthopédiques, on prend en charge parce que la Sécu reconnaît ce soin.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load model\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"bofenghuang/whisper-large-v2-cv11-french\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"bofenghuang/whisper-large-v2-cv11-french\", language=\"french\", task=\"transcribe\")\n",
    "\n",
    "# NB: set forced_decoder_ids for generation utils\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n",
    "\n",
    "# 16_000\n",
    "model_sample_rate = processor.feature_extractor.sampling_rate\n",
    "\n",
    "# Load data\n",
    "path = 'audio_samples/train_0003.wav'\n",
    "input_speech, sr = librosa.load(path, sr=None)\n",
    "sample_rate = sr\n",
    "\n",
    "# Resample\n",
    "if sample_rate != model_sample_rate:\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, model_sample_rate)\n",
    "    input_speech = resampler(input_speech)\n",
    "\n",
    "# Get feat\n",
    "inputs = processor(input_speech, sampling_rate=model_sample_rate, return_tensors=\"pt\")\n",
    "input_features = inputs.input_features\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(inputs=input_features, max_new_tokens=225)  # greedy\n",
    "# generated_ids = model.generate(inputs=input_features, max_new_tokens=225, num_beams=5)  # beam search\n",
    "\n",
    "# Detokenize\n",
    "generated_sentences = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T09:41:54.268108759Z",
     "start_time": "2023-05-05T09:39:16.267958004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
